# 3F1 Signals and Systems[^1]
[^1]:Created by: Tom Xiaoding  Lu on 10/04/18

## L1. Introduction and Recap
### 1.1 Euler Method for Solving ODE
* Given the gradient (rate of change) of some variable over time $\dfrac{dy}{dt}=f(y,t)$
* Define a fixed timestep $\delta t$. Then, ignoring initial conditions, we can derive an approximate solution for the next timestep:
  $$
  y(t+\delta t) = y(t)+\delta t \times \dfrac{dy}{dt} = y(t)+\delta t \times f(y, t)
  $$
### 1.2 Pros and Cons of Discrete Time Methods
* Examples using digital signal processors (Digital circuits) over the analogue/continuous (Analog electronics) processes are: digital audio, digital communication link, image processing and speech synthesis.
* Note Discrete Time is different from digital signals, **discrete time signals have quantized time interval but the output is not quantized, digital outputs are quantized**. Below are the advantages and disadvantages of discrete time methods:
<center>

  | Advantages  | Disadvantages|
  |:-------------:|:-------------:|
  |Algorithms can be implemented on any hardware | $\delta t$ determines the stability which is hard to guarantee|
  |Flexible and easy to modify| Need ADC/DAC hardware
  | Easy to implement complex control systems| Finite sampling rate

</center>
### 1.3 LTI Systems Revision
* A system is **linear** if it satisfies the Principal of Superposition:
  if $\mathcal{L}(u_1(t))=y_1(t) $ and $\mathcal{L}(u_2(t))=y_2(t)$, then for any scalars $\alpha_1, \alpha_2$:
  $$
  \mathcal{L}(\alpha_1 u_1 + \alpha_2 u_2) = \alpha_1 \mathcal{L}(u_1(t)) + \alpha_2\mathcal{L}(u_2(t)) = \alpha_1 y_1(t) + \alpha_2 y_2(t)
  $$
* A system is **time-invariant** if $\mathcal{L}(u(t))=y(t)$ then $\mathcal{L}(u(t+T))=y(t+t)$ for any time interval $T$
* Note nearly all real world systems are not time invariant, i.e. the structural integrity of a bridge changes over time.
* A system is *stable* if the transfer function $G(s)$ has no poles in the right half plane or on the imaginary axis.
* For a stable LIT system $G(s)$, the steady state response for an input $u(t) = \sin(\omega t)$ is:
$$
y_{ss}(t)=|G(j\omega)|\sin(\omega t+ \angle G(j\omega))
$$
* The Nyquist Stability Criterion gives a test for the closed loop system to be stable. Consider the following feedback system:
  * First plot the Nyquist diagram of $F(s)G(s)K(s)$, which is the locus of $F(j\omega)G(j\omega)K(j\omega)$ as $\omega$ varies from $-\infty$ through to 0 to $\infty$
  * Let $N$ be the number of anti-clockwise encirclements of the $-1/k$ point of the Nyquist diagram
  * Closed loop stability is achieved $\iff N = \textrm{\# of RHP poles of} \; F(s)G(s)K(s)$
<center>

![DeepinScreenshot_select-area_20181004170123](/assets/DeepinScreenshot_select-area_20181004170123.png)

</center>

* MATLAB Nyquist Implementation (Non library):
In order to plot the nyquist plot for $G(s) = \dfrac{1}{(s+2)(s+4)(s+6)}$:
```Matlab
% create symbolics and transfer functions
syms s w
G = 1/((s+2) * (s+4) * (s+6));

% substitute jw into the transfer function
G_w = subs(G, s, j*w);

% sweep across all frequencies
W = [-100 : 0.1 : 100];
Nyq = eval(subs(G_w, w, W));

% plot the real part against imaginary part
x = real(Nyq)
y = imag(Nyq)
```
* MATLAB Nyquist Implementation (Library)
```Matlab
% create transfer function
G = tf ([0 0 0 1],[1 12 44 48])
% plot with built in function
nyquist(G)
grid on
```
## L2. Z-transform
### 2.1 Introduction
We know Laplace transform maps a signal from time domain into $s$ domain, it operates on continuous signals $x(t)$. What about its equivalent for discrete signals?
A discrete time signal is a number sequence with discrete intervals $T$:
$$
[x(0), x(T), x(2T), ...] \;\; \textrm{or} \;\; [x_0,x_1,x_2,...]
$$
$T$ is also known as the **sampling period**, i.e. the intervals between two signals is the period of time we wait before we take a sample of the continuous signal $x(T)$. A standard notation is:
$$
\{x(kT)\}_{k\geq 0} \;\; \textrm{or} \;\; \{x_k\}_{k\geq 0}
$$
Analogous to Laplace transform, the z-transform for $$\{x(kT)\}_{k\geq 0}$$ is :
$$
\sum_{k=1}^\infty x(kT)e^{-skT} = \sum_{k=1}^\infty x(kT)z^{-k}
$$
We use $z=e^{sT}$ to make the notation easier as $e^{sT}$ always appears as a group.

**But what exactly is z-transform?** We recall from IB that $e^{sT}$ is equal to $e^{σ+jω}=re^{jω}$, now the important thing is to visualize possibilities for $z^{-k}=r^{-k}e^{-jωk}$. **k in this case is the index of our signal, we can literally interpret it as equivalent to time in a continuous signal**. Just like how we plot time varying functions, here $z^{-k}$ is an 'index' varying function with $r$ and $ω$ interpreted as the amplitude and frequency of some signal sampled with time period $T$. Figures below show a set of possible signals generated by $z^{-k}$:
<center>

![z_transform](/assets/z_transform.jpg)
</center>

So $z^{-k}$ generates all possible signals ranging from a train of ones ($r=0, ω=0$) to exponentially decaying and increasing sinusoids. This is what we mean by mapping a signal from $k$ to $s$ domain. But what does the dot multiplication mean for each $x(kT)z^{-k}$? Without diving into details, say if we have ${x(kT)}= [3, -3, 3, -3, ...]$ and $z_{|s=0}^{-k} = [1,1,1,1...]$, multiplying these together, we get $\sum_{k=1}^\infty x(kT)z_{|s=0}^{-k}=0$, the zero here can be interpreted as a form of correlation between two functions, i.e how much component there is in one function from another, or the response of the system had we excited it with that signal. Putting all these together, we see z-transform tells us the response of our system at all possible input signals, just like Laplace transform except we are mapping from $k$ to $s$.
