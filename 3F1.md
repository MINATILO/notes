# 3F1 Signals and Systems[^1]
[^1]:Created by: Tom Xiaoding  Lu on 10/04/18

## L1. Introduction and Recap
### 1.1 Euler Method for Solving ODE
* Given the gradient (rate of change) of some variable over time $\dfrac{dy}{dt}=f(y,t)$
* Define a fixed timestep $\delta t$. Then, ignoring initial conditions, we can derive an approximate solution for the next timestep:
  $$
  y(t+\delta t) = y(t)+\delta t \times \dfrac{dy}{dt} = y(t)+\delta t \times f(y, t)
  $$
### 1.2 Pros and Cons of Discrete Time Methods
* Examples using digital signal processors (Digital circuits) over the analogue/continuous (Analog electronics) processes are: digital audio, digital communication link, image processing and speech synthesis.
* Note Discrete Time is different from digital signals, **discrete time signals have quantized time interval but the output is not quantized, digital outputs are quantized**. Below are the advantages and disadvantages of discrete time methods:
<center>

  | Advantages  | Disadvantages|
  |:-------------:|:-------------:|
  |Algorithms can be implemented on any hardware | $\delta t$ determines the stability which is hard to guarantee|
  |Flexible and easy to modify| Need ADC/DAC hardware
  | Easy to implement complex control systems| Finite sampling rate

</center>
### 1.3 LTI Systems Revision
* A system is **linear** if it satisfies the Principal of Superposition:
  if $\mathcal{L}(u_1(t))=y_1(t) $ and $\mathcal{L}(u_2(t))=y_2(t)$, then for any scalars $\alpha_1, \alpha_2$:
  $$
  \mathcal{L}(\alpha_1 u_1 + \alpha_2 u_2) = \alpha_1 \mathcal{L}(u_1(t)) + \alpha_2\mathcal{L}(u_2(t)) = \alpha_1 y_1(t) + \alpha_2 y_2(t)
  $$
* A system is **time-invariant** if $\mathcal{L}(u(t))=y(t)$ then $\mathcal{L}(u(t+T))=y(t+t)$ for any time interval $T$
* Note nearly all real world systems are not time invariant, i.e. the structural integrity of a bridge changes over time.
* A system is *stable* if the transfer function $G(s)$ has no poles in the right half plane or on the imaginary axis.
* For a stable LIT system $G(s)$, the steady state response for an input $u(t) = \sin(\omega t)$ is:
$$
y_{ss}(t)=|G(j\omega)|\sin(\omega t+ \angle G(j\omega))
$$
* The Nyquist Stability Criterion gives a test for the closed loop system to be stable. Consider the following feedback system:
  * First plot the Nyquist diagram of $F(s)G(s)K(s)$, which is the locus of $F(j\omega)G(j\omega)K(j\omega)$ as $\omega$ varies from $-\infty$ through to 0 to $\infty$
  * Let $N$ be the number of anti-clockwise encirclements of the $-1/k$ point of the Nyquist diagram
  * Closed loop stability is achieved $\iff N = \textrm{\# of RHP poles of} \; F(s)G(s)K(s)$
<center>

![DeepinScreenshot_select-area_20181004170123](/assets/DeepinScreenshot_select-area_20181004170123.png)

</center>

* MATLAB Nyquist Implementation (Non library):
In order to plot the nyquist plot for $G(s) = \dfrac{1}{(s+2)(s+4)(s+6)}$:
```Matlab
% create symbolics and transfer functions
syms s w
G = 1/((s+2) * (s+4) * (s+6));

% substitute jw into the transfer function
G_w = subs(G, s, j*w);

% sweep across all frequencies
W = [-100 : 0.1 : 100];
Nyq = eval(subs(G_w, w, W));

% plot the real part against imaginary part
x = real(Nyq)
y = imag(Nyq)
```
* MATLAB Nyquist Implementation (Library)
```Matlab
% create transfer function
G = tf ([0 0 0 1],[1 12 44 48])
% plot with built in function
nyquist(G)
grid on
```
## L2. Z-transform
### 2.1 Introduction
We know Laplace transform maps a signal from time domain into $s$ domain, it operates on continuous signals $x(t)$. What about its equivalent for discrete signals?
A discrete time signal is a number sequence with discrete intervals $T$:
$$
[x(0), x(T), x(2T), ...] \;\; \textrm{or} \;\; [x_0,x_1,x_2,...]
$$
$T$ is also known as the **sampling period**, i.e. the intervals between two signals is the period of time we wait before we take a sample of the continuous signal $x(t)$. A standard notation is:
$$
\{x(kT)\}_{k\geq 0} \;\; \textrm{or} \;\; \{x_k\}_{k\geq 0}
$$
Analogous to Laplace transform, the z-transform for $$\{x(kT)\}_{k\geq 0}$$ is :
$$
\sum_{k=1}^\infty x(kT)e^{-skT} = \sum_{k=1}^\infty x(kT)z^{-k}
$$
We use $z=e^{sT}$ to make the notation easier as $e^{sT}$ always appears as a group.

**But what exactly is z-transform?** We recall from IB that $e^{sT}$ is equal to $e^{σ+jω}=re^{jω}$, now the important thing is to visualize possibilities for $z^{-k}=r^{-k}e^{-jωk}$. **k in this case is the index of our signal, we can literally interpret it as equivalent to time in a continuous signal**. Just like how we plot time varying functions, here $z^{-k}$ is an 'index' varying function with $r$ and $ω$ interpreted as the amplitude and frequency of some signal sampled with time period $T$. Figures below show a set of possible signals generated by $z^{-k}$:
<center>

![DeepinScreenshot_select-area_20181010013818](/assets/transform.png)
</center>

So $z^{-k}$ generates all possible signals ranging from a train of ones ($r=0, ω=0$) to exponentially decaying and increasing sinusoids. This is what we mean by mapping a signal from $k$ to $s$ domain. But what does the dot multiplication mean for each $x(kT)z^{-k}$? Without diving into details, say if we have ${x(kT)}= [3, -3, 3, -3, ...]$ and $z_{|s=0}^{-k} = [1,1,1,1...]$, multiplying these together, we get $\sum_{k=1}^\infty x(kT)z_{|s=0}^{-k}=0$, the zero here can be interpreted as a form of correlation between two functions, i.e how much component there is in one function from another, or the response of the system had we excited it with that signal. Putting all these together, we see z-transform tells us the response of our system at all possible input signals, just like Laplace transform except we are mapping from $k$ to $s$.

### 2.2 Properties of Z-Transform
* **Linearity**
  $$
  \mathcal{Z}(α_{x_k}+β{y_k})=α\mathcal{Z}(\{x_k\}) + β\mathcal{Z}(\{y_k\})
  $$
* **Time delay**
  States a time delay equals to multiplying by $z^{-1}$ in z domain, for a signal $\{x_k\}$:
  $$
  \{x_k\} = (x_0,x_1,x_2,...) → \{x_{k-1}\} = (x_{-1},x_0,x_1,...)
  $$
  Start by writing a few terms down:
  $$
  \mathcal{Z}(\{x_{k-1}\}) = \sum_{k=1}^∞ x_{k-1}z^{-k}=x_{-1}+x_{0}z^{-1}+x_{1}z^{-2}+x_{2}z^{-3}+...
  $$
  $$
  \mathcal{Z}(\{x_{k-1}\}) = x_{-1}+\sum_{i=0}^{∞}x_i z^{-i-1}=x_{-1}+z^{-1}X(z)
  $$
* **Time advance**
  Also starts by writing down a few terms:
  $$
  \mathcal{Z}(\{x_{k-1}\}) = \sum_{k=1}^∞ x_{k+1}z^{-k}=x_{1}+x_{2}z^{-1}+x_{3}z^{-2}+x_{4}z^{-3}+...
  $$
  $$
  \mathcal{Z}(\{x_{k-1}\}) = \sum_{i=1}^{∞}x_i z^{-i+1} = -x_0z+z\sum_{i=0}^{\infty}x_i z^{-i} = -zx_0 +z X(z)
  $$
* **Initial value theorem**
  $$
  x_0 = \lim_{z→∞}(x_0+x_1/z+x_2/z^2+...)=\lim_{z→∞}X(z)
  $$
* **Convolution**
  This one actually requires a bit more thought. Let us write down two signals $\{x_k\}=[1,2,3,4,5,6]$ and $\{y_k\}=[7,8,9,10,11,12]$, what is ${x_k}⋆y_k$? Let us also have some naming convention, $\{x_k\}$ and $\{y_k\}$ are *signals*, their contents are called *events* indexed by $n$, i.e. for signal $x$, at $n=0$, there is an event $x[0]=1$. When convolving the two signals together, we wish to know $\{z_k\}$ with each event denoted by $z[n]$. This is shown in the picture below:
  <center>

  ![markov - Page 3](/assets/markov%20-%20Page%203_91rxij1d7.png)  
  </center>

  So what is each event $z[n]$ in out output $z$? Let is first define what convolution is mathematically:
  $$
  z[n]=\sum_{i=0}^{n}x_n y_{n-i}
  $$
  Picture below shows how $z[4]$ is calculated:
  <center>

  ![markov - Page 4 (4)](/assets/markov%20-%20Page%204%20(4).png)
  </center>

  If we think about what this actually means, at the index $4$, the result $z_4$ is due to the fact that the latest event $x_4$ triggered the initial response $y_0$, the previous event $x_3$ is now triggering $y_1$ as one time interval has passed, same goes for all previous events. The outcome $z_4$ is the combination of previous events' residual effects plus current event's immediate effect. Finally, we are able to write, for our train of outputs $\{z_k\}$:
  $$
\{z_k\}=\{x_k\} ⋆ \{y_k\} = \sum_{i=0}^{k}x_k y_{k-i}
  $$
  Which is just the previous equation but for every single event up to $k$. Also for the picture above, the same result could be obtained by swapping $x$ and $y$ signals' order, hence we can also write:

  $$
  \{z_k\} = \sum_{i=0}^{k}x_{k-i} y_{i}
  $$
The $z$ transform of a convolution requires swapping the order of summation, the derivation and "helper" diagram are displayed below, note all we are doing is instead of summing from left to right, we index from bottom to top.

<center>

![DeepinScreenshot_select-area_20181012013842](/assets/DeepinScreenshot_select-area_20181012013842.png)
</center>
