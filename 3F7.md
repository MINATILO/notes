# 3F7 Information Theory and Coding[^1]
[^1]:Created by: Tom Xiaoding  Lu on 10/07/18
## L1. Probability Review
### 1.1 Random Variables
* A random variable (rv) $X$ is a function that maps outcome of experiment to value in set $\mathcal{X}$
* It is characterised by a probability mass function (pmf), $P_X(x)$ is the *realisation* of the rv $X$
* It has a cumulative distribution function (cdf), $F_X(a)=Pr(X\leq a)=\sum_{x\leq a}P_X(x)$
* Expected value is like the center of mass of the pmf, $\mathcal{E}(X)=\sum_a aP_X(a)$
* Variance is $\mathcal{E}[(X-\mathcal{E}X)^2]$, think of it as the expectation of the deviation from the mean
* $\mathcal{E}[(X-\mathcal{E}X)^2] = \mathcal{E}[X^2-2X\mathcal{E}(X)+(\mathcal{E}X)^2]=\mathcal{E}(X^2)-2\mathcal{E}(X)\mathcal{E}(X)+(\mathcal{E}X)^2=\mathcal{E}(X^2)-(\mathcal{E}X)^2$
* For any constant $a$, $\textrm{Var}(aX)=a^2\textrm{Var}(X)$
* Often we take expectations of functions of rvs, e.g. $\mathcal{E}[g(x)]=\sum_ag(a)P_X(a)$
### 1.2 Jointly Distributed Discrete rvs
Characterised by *joint pmf* $P_{XY}(x,y),\;\;x\in X,y\in Y$
*Marginal distributions:* $P_X$ can be computed through summation over $Y$:
$$
P_X(x)=\sum_y P_{XY}(x,y)
$$
*Conditional distribution* of $Y$ given $X$, we divide the joint distribution by what we already know (the distribution of $X$):
$$
P_{Y|X}(y|x)=\dfrac{P_{XY}(x,y)}{P_X(x)}
$$
**Product rule** is simply a rule derived from following the probability tree, think of it as generating a chain of probabilities:
$$
P_{XYZ}=P_X P_{Y|X} P_{Z|XY}=P_Y P_{X|Y} P_{Z|XY}\; \textrm{,etc}
$$
**Sum rule** is just a generalised form of marginalisation in multivariables:
$$
P_X(x)=\sum_y \sum_z P_{XYZ}(x,y,z)
$$
**Independence** is achieved within discrete random variables $X_1,X_2,...,X_n$ if and only if:
$$
P_{X1...X_n}(x1,...,x_n)=\prod_{j=1}^{n} P_{X_j}(x_j)
$$
Recall that in product rule, we can write the equation above as a generating function:
$$
P_{X_1,...,X_n}(x_1,...,x_n)=P_{X_1}(x_1)P_{X_2|X_1}(x_2|x_1)...P_{X_n|X_{n-1}...X_1}(x_n|x_{n-1}...x_1)
$$
Thus $X_1,X_2,...,X_n$ only when
$$
P_{X_i|\{X_j\}_{j\neq i}}= P_{X_i}
$$
### 1.3 Continuous Random Variables
* Characterised by the joint *density* function $f_{XY}(x,y)$
* $Pr(a\leq X \leq b, c\leq Y \leq d)=\int_{x=a}^{b}\int_{y=c}^{d}f_{XY}(x,y)dxdy$
* Conditional density, product and sum rules are analogous to discrete case, just replace the sum with integrals
## L2. Entropy Review
Entropy can be thought as the minimum number of bits required to encode some events that follow a probability distribution. The entropy of a discrete random variable $X$ with pmf $P$ is:
$$
H(X)=\sum_{x}P(x)\log \dfrac{1}{P(x)}
$$
### 2.1 Binary Entropy Function
$X$ is called a Bernoulli(p) random variable if takes value 1 with probability $p$ and $0$ with probability $1-p$. Intuitively, it is most uncertain when $p=0.5$ and has zero entropy at $p=\{0,1\}$. Its entropy's analytical equation is simply:
$$
H_2(p)=p\log \dfrac{1}{p} + (1-p)\log \dfrac{1}{1-p}
$$
This is plotted below:
<center>

![bernouli](/assets/bernouli.jpg)
</center>
### 2.2 Properties of Entropy
Putting it to words, the entropy of a random variable is **non negative** and bounded by the entropy of a uniform distribution.
Let $X$ be discrete random variable taking values in $\mathcal{X}$. Denote the alphabet size $|\mathcal{X}|$ by $M$. Then:
* $H(X)\geq 0$
* $H(X)\leq \log M$
* Among all random variables taking values in $\mathcal{X}$, the uniform distribution has the maximum entropy equal to $\log M$
#### 2.2.1 Greater than 0 proof
For any $x\in X,\; 0\leq P(x) \leq 1$, so $\dfrac{1}{P(x)}\geq 1$, therefore $\log \dfrac{1}{P(x)}\geq 1$ Hence:
$$
H(X) = \sum_{x}P(x)\log \dfrac{1}{P(x)}\geq 1
$$
#### 2.2.2 Upper Bound Proof
Need to use the property $\ln x \leq  (x-1)$, for $x > 0$ with equality iff $x=1$
$$
H(X)-\log M = \sum_xP(x)\log \dfrac{1}{P(x)}-\sum_xP(x)\log M
$$
$$
=\dfrac{1}{\ln2}\sum_xP(x) \ln \dfrac{1}{MP(x)} \leq \dfrac{1}{\ln2}\sum_xP(x)  \left(\dfrac{1}{MP(x)}-1\right)
$$
$$
= \dfrac{1}{\ln2}\left(\sum_x \dfrac{1}{M}- \sum_xP(x)\right)=0
$$
We also know that for equality to happen $\dfrac{1}{MP(x)}=1$ therefore $P(x)=\dfrac{1}{M}$
### 2.3 Joint and Conditional Entropy
For the **joint entropy** of discrete rvs $X$ and $Y$, think of the matrix of the probability mass function $P_{XY}$, its entropy is just the sum of the elements times the minus log of the elements within this probability matrix:
$$
H(X,Y)=\sum_{x,y}P_{XY}(x,y)\log \dfrac{1}{P_{XY}(x,y)}
$$
The **conditional entropy** of $Y$ given $X$ is sort of counter intuitive written in its raw form:
$$
H(Y|X)=\sum_{x,y}P_{XY}(x,y) \log \dfrac{1}{P_{Y|X}(y|x)}
$$
Let us try to prove its relationship with the joint entropy to gain some intuiation:
$$
H(Y|X)=\sum_{x,y}P_{XY}(x,y) \log \dfrac{P_X(x)}{P_{XY}(x,y)}
$$
$$
H(X) =  \sum_{x}P_X(x)\log \dfrac{1}{P_X(x)} =  \sum_{x,y}P_{XY}(x,y)\log \dfrac{1}{P_X(x)}
$$
$$
H(X)+H(Y|X)=  \sum_{x,y} P_{XY}(x,y) \left( \log \dfrac{P_X(x)}{P_{XY}(x,y)}+\log \dfrac{1}{P_X(x)} \right) =  \sum_{x,y} P_{XY}(x,y) \log \dfrac{1}{P_{XY}(x,y)} = H(X,Y)
$$
Hence, we arrive at a more intuitive result where uncertainty (entropy) of $Y$ given $X$ is equal to the joint uncertainty minus the uncertainty in $X$ since it is "known".
$$
H(Y|X)=H(X,Y)-H(X)
$$
Also the conditional entropy $H(Y|X)$ can also be written as the *average* uncertainty in $Y$ given $X$ using product rule:
$$
H(Y|X)=\sum_{x}P_X(x)\sum_y P_{Y|X}(y|x) \log \dfrac{1}{P_{Y|X}(y|x)} = \sum_{x}P_X(x)H(Y|X=x)
$$
### 2.4 Joint Entropy of Multiple RVs
The **joint entropy** of $X_1,X_2,...,X_n$ is:
$$
H(X_1,X_2,...,X_n) = \sum_{x_1,...,x_n}P_{X_1,...X_n}(x_1,...x_n)\log\dfrac{1}{P_{X_1,...X_n}(x_1,...x_n)}
$$
Which can be written in a generative form, interpreted as total uncertainty is uncertainty in $X_1$ plus uncertainty in $X_2$ knowing $X_1$, plus uncertainty in $X_3$ knowing both $X_1$ and $X_2$ and so on:
$$
H(X_1,X_2,...,X_n) = H(X_1)+H(X_2|X_2)+H(X_3|X_1,X_2)+...+H(X_n|X_1,...X_{n-1})
$$
Equation above can be written in a condensed format:
$$
H(X_1,X_2,...,X_n) = \sum_{i=1}^{n}H(X_i|X_{i-1},...,X_1)
$$
If $X_1,X_2,...,X_n$ are all independent then the joint entropy is the sum of their individual entropy:
$$
H(X_1,X_2,...,X_n) = \sum_{i}^{n}H(X_i)
$$

## L3. Law of Large Numbers
### 3.1 Estimating Tail Probabilities
We often want to bound the probability of unlikely events, sometimes we do not know the distribution of the rv, but we can still get a bound for it using:
* **Markov's inequality** for **non negative rvs** which requires only mean
* **Chebyshev's inequality** which requires both mean and variance (for general rvs)

#### 3.1.1 Markov's Inequality
**Mechanical Intuiation:**
Let us imagine the probability mass function being some sort of shape, this is especially valid since we have only positive $rv$s:
<center>

![DeepinScreenshot_select-area_20181018173734](/assets/DeepinScreenshot_select-area_20181018173734.png)</center>
</center>

So the center of mass is at $\mu$. Let us denote a region of mass $M$ contributed by axis $[a, \infty)$, we can also write $M$ as:
$$
M = P(X\geq a)
$$
We can write down with that $\mu \geq Ma$ since at best we have $M$ being a point mass that lies exactly $\mu$, otherwise it will bring the center of mass up. Therefore we arrive at the Markov's inequality:
$$
P(X\geq a)\leq \dfrac{\mathcal{E}[X]}{a}
$$
**Proof of Markov's Inequality:**
We start with the definition of expectation and break it into two constituents:
$$
\mathcal{E}[X]=\sum_{r\geq 0}rP(X=r) = \sum_{r< a}rP(X=r)+ \sum_{r\geq a}rP(X=r)
$$
Since for the part where $r\geq a$, we can write $\sum_{r\geq a}rP(X=r) \geq \sum_{r\geq a}aP(X=r)$, therefore:
$$
\mathcal{E}[X] \geq \sum_{r< a}rP(X=r) + \sum_{r\geq a}aP(X=r)
$$
$$
\mathcal{E}[X] \geq a \sum_{r\geq a}P(X=r) = a P(X\geq a)
$$

#### 3.1.2 Chebyshev's Inequality:
Often, we need to bound the tail probabilities of *deviations* around the mean of an *rv*, i.e. working out the margin of safety using:
$$
P(|X- \mathcal{E}[X]|\geq a), \;\; \textrm{for some }a>0
$$
**Chevyshev's inequality is a way to bound the probability of deviations around the mean** with knowledge of just the *mean* and *variance* $\sigma^2$.
$$
P(|X- \mathcal{E}[X]|\geq a) \leq \dfrac{\sigma^2}{a^2}
$$
**Mechanical Intuition:**
In order to build some sort of intuition for it, we imagine our probability mass function being some sort of physical shape:
<center>

![DeepinScreenshot_select-area_20181018165552](/assets/DeepinScreenshot_select-area_20181018165552.png)
</center>

Looking at the picture above, we say that the variance $\sigma ^2 = \mathcal{E}[(X-\mu)^2] = \int_{-\infty}^{\infty}dx f(x)(\mu-x)^2 $ is interpreted as the *moment of inertia* of our "shape" spinning about the center of rotation $\mu$. If we denote $M$ being the total mass contributed by the region $(-\infty,\mu-k\sigma] \cup [\mu+k\sigma, \infty)$ as shown, we have:
$$
M = P(|X- \mu|\geq k\sigma) = P(|X- \mathcal{E}[X]|\geq k\sigma)
$$
Now, we know when this shape is spinning, to calculate its moment of inertia, our region $M$ will contribute at least $M (k\sigma)^2$ by imagine them being point masses distance $k\sigma$ away from the centre $\mu$. Therefore we can write down:
$$
\sigma^2 \geq M (k\sigma)^2
$$
Hence
$$
1 \geq M k^2 = P(|X- \mathcal{E}[X]|\geq k\sigma) k^2
$$
We arrive at the alternative form of *Chebyshev's* inequality (origional form is obtained through substituting $a=k\sigma$):
$$
P(|X- \mathcal{E}[X]|\geq k\sigma) \leq \dfrac{1}{k^2}
$$

**Formal Proof**:
Note that $$P(|X-\mu|\geq a) = P(|X-\mu|^2 \geq  a^2)$$
Let $Y = |X-\mu|^2$, since $Y$ is a non-negative rv, Markov's inequality can be applied:
$$
P(Y\geq a^2)\leq \dfrac{\mathcal{E}[Y]}{a^2} \rightarrow P(|X-\mu|\geq a) \leq  \dfrac{\textrm{Var}(X)}{a^2}
$$
### 3.2 Weak Law of Large Numbers
Very intuitive, as it basically states empirical average converges to mean.
Let $X_1,X_2, ...$ be a sequence of i.i.d (independent and identically distributed) rvs with finite mean $\mu$. Let $S_n=\dfrac{1}{n}\sum_{i=1}^n X_i$ then **informally**:
$$
\lim_{n\rightarrow \infty}S_n\rightarrow\mu
$$
Formally:
$$
\lim_{n\rightarrow \infty}P(|S_n-\mu|\geq \epsilon)=0
$$
**Proof**:
Using Chebyshev's inequality we write:
$$
P(|S_n-\mu|\geq \epsilon)\leq \dfrac{\textrm{Var}(S_n)}{\epsilon^2}
$$
Where  $\textrm{Var}(S_n) = \dfrac{1}{n^2}\textrm{Var}(\sum_i X_i)^*  = \dfrac{1}{n^2}\sum_i \textrm{Var}(X_i)= \dfrac{n\sigma^2}{n^2} $
Hence:
$$
P(|S_n-\mu|\geq \epsilon)\leq \dfrac{\sigma^2}{n^2}
$$
Taking the limit to $n\rightarrow\infty$ we have $P(|S_n-\mu|\geq \epsilon)=0$

\*I was quiet sceptical about the line $\textrm{Var}(\sum_i X_i)=\sum_i \textrm{Var}(X_i)$, this intuitively does make sense for i.i.d rvs (total uncertainty equals to sum of individual uncertainty contributions) but lets still prove it, for two i.i.d rvs $X$ and $Y$:
$$
\textrm{Var}(X+Y)=\mathcal{E}[(X+Y)^2-(\mathcal{E}[X+Y])^2]=\mathcal{E}[(X+Y)^2-(\mathcal{E}[X]+\mathcal{E}[Y])^2]
$$
$$
\textrm{Var}(X+Y)=\mathcal{E}[X^2+Y^2+2XY-\mathcal{E}[X]^2-\mathcal{E}[Y]^2-2\mathcal{E}[X]\mathcal{E}[Y]]
$$
$$
\textrm{Var}(X+Y)=\mathcal{E}[X^2]+\mathcal{E}[Y^2]+2\mathcal{E}[XY]-\mathcal{E}[X]^2-\mathcal{E}[Y]^2-2\mathcal{E}[XY]
$$
$$
\textrm{Var}(X+Y)=\textrm{Var}(X)+\textrm{Var}(Y)
$$
### 3.3 Asymptotic Equipartition Property (AEP)
This property gives us more insight the meaning of entropy. We have the intuition that for whatever r.v, if we generate a long sequence of it, the probability of different sequences should be the same. i.e let the rv be english words, an article being the generated sequence of such rvs, the note I am typing now has basically the same existing probability as a wikipedia article on dolphins! (This example is not so true since each word is dependent on the previous, but it demonstrates the idea).
But what does this probability tends towards? A very small number, but how small? The property of AEP dictates exactly how small this probability is, if $X_1,X_2,...$ are i.i.d rvs :
$$
\lim_{n\rightarrow \infty}\dfrac{-1}{n}\log P_X(X_1,X_2,...X_n)=H(X)
$$
Or in words: the log likelihood of a sequence tends towards its entropy for large sequences.

**Proof (Badly explained here):**
This is basically an extension of WLLN where $S_n=\dfrac{1}{n} \sum_i Y_i$:
$$
\lim_{n\rightarrow\infty}P(|S_n-\mathcal{E}[Y_i]|\geq \epsilon)=0\rightarrow  P(|S_n-\mathcal{E}[Y_i]| < \epsilon)=1
$$
From the log of the generated sequence $\log P_X(X1,X_2,...X_n)$ we have:
$$
\log \prod_{i=1}^n P_X(X_i) = \sum_{i=1}^n \log P_X(X_i)
$$
Let $\;\; -\sum_{i=1}\log P_X(X_i) = \sum_{i=1}Y_i\;$ and substitute into WWLN we get:

$$
\lim_{n\rightarrow\infty}P(|\dfrac{-1}{n}\log P_X(X_1,X_2,...X_n)-\mathcal{E}[Y_1]|< \epsilon)=1
$$
Noting the expectation of $Y_1=\log P_X(X_1)$ is $H(X)$ therefore:
$$
\lim_{n\rightarrow\infty}P\left(\left|\dfrac{-1}{n}\log P_X(X_1,X_2,...X_n)-H(X)\right|< \epsilon\right)=1
$$

## L4. Prefix-Free Codes and Kraft Inequality
### 4.1 Prefix-Free Codes
A code is *prefix-free* if **no codeword is the prefix of another**, i.e. {0,10,11} is a prefix code, {00,010,011,0101,1} is not since 010 is a prefix for 0101. In order to decode a prefix code, we would have to use letter spaces.
Graphically, the prefix condition on a tree has **codewords on the leaves of the tree** in the diagram below nodes that are coloured are the codewords:
<center>

![DeepinScreenshot_select-area_20181020221902](/assets/DeepinScreenshot_select-area_20181020221902.png)
</center>

### 4.2 Kraft Inequality
* A full tree of depth $l_{\max}$ has $2^{l_{\max}}$ leaves at that depth level
* Whenever a codeword of lenthg $l<l_{\max}$ is created, all its descendants are removed from the tree

<center>

![DeepinScreenshot_select-area_20181020223121](/assets/DeepinScreenshot_select-area_20181020223121.png)
</center>

* Because a codeword **must** be a leaf which is at the termination of a node. Therefore each codeword of length $l$ leads to a reduction of $2^{l-l_{\max}}$ leaves at the maximum depth $l_{\max}$
* Hence the total number of unusable leaves at level $l_{\max}$ for a code with word length {$l_1,l_2,...l_N$} is $\sum_{i=1}^N 2^{l_{max}-l_i}$
* We arrive at the Kraft inequality which is a necessary condition for any prefix-free code with lengths {$l_1,...l_N$}:
$$
\sum_{i=1}^N 2^{l_{max}-l_i} \leq 2^{l_{\max}} \quad \rightarrow \quad \sum_{i=1}^N 2^{-l_i} \leq 1
$$

### 4.3 Efficient Coding Theorem
The expected codeword length $L$ is simply $L=\sum_{x\in \mathcal{X}} p(x)l(x)$ letting $X$ be a random variable taking values in the alphabet $\mathcal{X}$ with entropy $H(X)$. Then for any binary prefix-free code of $X$ satisfies:
$$
L \geq H(X)
$$
To prove this, we need to show that $H(X)-L \leq 0$:
<center>

![DeepinScreenshot_select-area_20181020230050](/assets/DeepinScreenshot_select-area_20181020230050.png)
</center>

where $(a)$ is obtained using inequality $\ln(x)\leq x-1$ and $(b)$ uses Kraft inequality.
For blocks of source symbols, if $X$ is an iid source, denoting $X^N = \{X_1,X_2,...,X_N\}$ then:
$$
H(X^N) = NH(X) \quad \rightarrow \quad \dfrac{\mathcal{E}[l(X^N)]}{N}\geq H(X)
$$

## L7. Block Codes
### L7.1 Binary Linear Block Codes
* **Minimum distance of a code:**
  - The Hamming distance $d(x,y)$ between two binary sequences $x, y$ of length $n$ is the number of positions in which $x$ and $y$ differ.
  - The optimal decoder for BSC picks the codeword closest in Hamming distance to $y$ (the recieved binary sequence)
* **BSC optimal decoding**
  - Can successfully correct any pattern of $t$ errors if $t \leq \lfloor \dfrac{d_{min}-1}{2}\rfloor$
  - High dimensional spheres centered around the code word $c_i$, the minimum distance between spheres is $d_{min}$, each sphere has radius $t$. Spheres will not overlap if $2t<d_{min}$ or $2t \leq d_{min} -1$
  ![Capture](/assets/Capture_q22l9z6w5.PNG)
* **Linear Block Codes**
  - A $(n,k)$ linear bloc code (LBC) contains a $k\times n$ generator matrix, mapping a $1\times k$ message to a $1\times n$ codeword. Each code word is therefore a linear combination of the row space of the generator matrix. $c = x_1\hat{g_1} + x_2\hat{g_2} +... + x_k\hat{g_k}$
  $G =  \begin{bmatrix}
  g_{1,1} & g_{1,2} & ... & ... & g_{1,n} \\
  g_{2,1} & g_{2,2} & ... & ... & g_{1,n} \\
  ... & ... & ... & ... & ... \\
  g_{k,1} & g_{k,2} & ... & ... & g_{k,n}
  \end{bmatrix}  $
  - Among all possible generator matricies for a code, often the form $G = [I_k | P]$ is prefered, such $G$ is called a systematic generator matrix.
  - Given any generator matrix, a systematic version can be found by swapping rows and replace any row by a sum of that row and any other rows
  - $ G_2 =  \begin{bmatrix}
1 & 1 & 1 & 0 \\
0 & 1 & 1 & 1
\end{bmatrix} \rightarrow_{R_1=R_1+R_2}\rightarrow G_1 =   \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 1 & 1
\end{bmatrix}   $
  - May also need to swap columns, this leads to rearranging the components of the codewords via a permutation matrix.
  - The sum of any two codewords is also a codeword; the al-zero vector is also a codeword, hence $\mathcal{C}$ is a subspace of $\{0,1\}^n$, where $\mathcal{C}$ is an $(n,k)$ LBC, this is shown through:
  $c_i=a_1\hat{g_1} + a_2\hat{g_2} + ... + a_k\hat{g_k}$
  $c_j = b_1\hat{g_1} + b_2\hat{g_2} + ... + b_k\hat{g_k}$
  $c_j+c_i = (a_1+b_1)\hat{g_1} + ... + (a_k+b_k)\hat{g_k} \in \mathcal{C}$
  $c_i + c_j = \hat{0} \in \mathcal{C}$

* **The parity check matrix**
  - The parity check matrix $H$ contains the basis for the orthogonal complement of $\mathcal{C}$ denoted $\mathcal{C}^{\perp}$
  - There are $k$ basis vectors in $\mathcal{C}$ of length $n$, hence there are $n-k$ basis vectors in $\mathcal{C}^{\perp}$
  - Thus the dimension for $H$ is $(n-k) \times n$
  - Given systematic $G = [I_k | P]$, $H=[P^T|I_{n-k}]$
  - $GH^T = [I_k|P]\begin{bmatrix}
P \\
I_{n-k}
\end{bmatrix} = P + P = 0$
  - i.e. The binary dot product of a codeword with each row of H is 0: $cH^T=0$
- **Minimum Distance of LBC is minimum hamming weights proof**
  - $d(u,v)=wt(u+v)$: Hamming distance between two binary vectors is the number of ones in their sum
  - $d_{min} = \min_{i\neq j} d(c_i,c_j) = wt(c_i + c_j) = wt(c_k)$
  - Noting sum of two codeword is another codeword
  - **The minimum distance of $\mathcal{C}$ is the smallest number of columns of H that sum to 0**
    - $\hat{c}H^T=0$
    - Above can be written as: $c_1H^T_1 + c_2H^T_2 + ... + c_nH^T_n = 0$ where $c_1$ demotes left most codeword bit, $H_1^T$ denotes first row of the transpose of parity matrix
    - Since $d_{min} = \min_{k}wt(\hat{c}_k)$, i.e. the code word with least numbers of 1 in it, we arrive at the conclusion that $d_{min}$ is the least number of $H_k^T$ which add together to 0.
    - Hence $d_{min}$ is the smallest number of columns of $H$ that sums to 1.
- **Performance of LBCs over BSC:**
  - Error patterns of weight up to $t = \lfloor\dfrac{d_{min}-1}{2}\rfloor$ can be corrected. Hence:
  - $P_e \leq 1- \sum_{w=0}^{t} {n \choose w} p^w (1-p)^{n-w}$

### L7.2 LDPC Codes
* The capacity of BEC is $1-\epsilon$. For a (6, 16) LBC, when the receiver receives $\hat{y} = [1xxxx10001x1xxx0]$ what is the decoded word?
* **Iterative method**
  - Spot the row of $H$ with only a single 1 in the erased bits
  - Set the erased bits as a variable to be solved
  - Repeat (for first iteration shown below, c =0)
  ![Capture](/assets/Capture_9i6s4xe4l.PNG)
  - This method however, does not always work
* **Regular LDPC Matrix Degree Distributions**
  - Each row of a LDPC matrix represents a parity check equation
  - Denote number of 1s in each row as $d_c$, each $(n-k)$ parity check equations involves $d_c$ code bits
  - Each column represents which parity check equation the code bit is in (1st column for the left most bit), denote number of 1s in each column as $d_v$, each codeword bits is involved in $d_v$ parity check equations
  ![Capture](/assets/Capture_a0lf2mbgt.PNG)
  - For a regular LDPC i.e. constant $d_v, d_c$, the **total number of 1s in the matrix** is $d_v n = d_c (n-k)$
  - Hence the design rate of a regular LDPC is $1- \dfrac{d_v}{d_c}$
* **Irregular LDPC Degree Distribution**
  - Representing each of the $(n-k)$ check bits equations as a square and each variable bit (codeword bit) as a circle, a factor graph is created representing the parity check matrix:

  ![Capture](/assets/Capture_b56o2hhv1.PNG)
- Two types of representations, node and edge representations can be used to represent an irregular LDPC matrix:

|                        | Node Representation                                                                                       | Edge Representation                                                                                           |
|------------------------|-----------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|
| Variable (column)      | $L_i$ **Fraction of variable nodes in H columns** of degree i                                             | $\lambda_i$ **Fraction of ones (edges) in H columns** of degree i                                             |
| Check (row)            | $R_i$ **Fraction of check nodes in H rows** of degree i                                                   | $\rho_i$ **Fraction of ones (edges) in H rows** of degree i                                                   |
| Degree distributions   | $$L(x)=\sum_{i=d_v,\min}^{d_v,\max}L_ix^i$$ $$R(x)=\sum_{i=d_c,\min}^{d_c,\max}R_ix^i$$                   | $$\lambda(x)=\sum_{i=d_v,\min}^{d_v,\max}\lambda_ix^{i-1}$$ $$\rho(x)=\sum_{i=d_c,\min}^{d_c,\max}\rho_ix^{i-1}$$ |
| $\bar{d_v}, \bar{d_c}$ | $$\bar{d_v}=\sum_{i=d_v,\min}^{d_v,\max}iL_i = L'(1)$$ $$\bar{d_c}=\sum_{i=d_c,\min}^{d_c,\max}iR_i = R'(1)$$ | $$\bar{d_v} = (\int_0^1 \lambda(x)dx)^{-1}$$ $$\bar{d_c} = (\int_0^1 \rho(x)dx)^{-1}$$  |

* **Iterative decoding as message passing**
  - Each variable node $v$ sends a message to each check node $c$ that it is conncected to
  - Each check node $c$ sends a message to each variable nodes that it is connected to
  - $m_{vc} = \begin{cases} x \in \{0,1\}\ \text{if v=x or at least one incoming } m_{c'v}=x \\ ?
   \qquad \text{if v=? and ALL incoming }m_{c'v}=? \end{cases}$
  - $m_{cv} = \begin{cases} \sum_{v'}m_{v'c} \text{under modulo addition 2, if no }m_{v'c}=?\\
  ? \qquad \text{  if at least one incoming }m_{v'c}=? \end{cases}$
  - These are demonstrated through the giff below, note dashed connections represent ?, light gray represents 0 and solid black represents 1 being transmitted

  ![Imgur](https://i.imgur.com/YPPRYFo.gif)

* **Predict good LDPC designs through Density Evolution**
  - The standard way to construct an LDPC code is to first choose a degree distribution for the code $(\lambda(x),\rho(x))$
  - Then fix a large code length n, and pick an H with this degree distribution, either at random or through some deterministic construction
  - **The density evolution recursion predicts the fraction of erased bits at the end of each step $t$**
    - First consider regular LDPC codes with $d_v=3, d_c=6$ (i.e. $\lambda(x)=x^2, \rho(x)=x^5$)
    - Let $p_t$ denote the probability that an outgoing $v\rightarrow c$ message is an erasure ? in step $t$.
    - Let $q_t$ denote the probability that a message $c\rightarrow v$ is an erasure in step $t$
    - $p_0 = \epsilon$ since the probability of the first variable taking on erasure is $\epsilon$ for BEC channel
    - $q_0=1$ since the first check to variable message is always an erasure
    - For $t\geq1$, assuming all the incoming messages at each node are independent, the probability that the **variable AND all $d_v-1$ messages are erasures is**:
    $$p_t = p_0 (q_{t-1})^{d_v-1} = \epsilon (q_{t-1})^{d_v-1}$$
    - The probability that **At least one of the $d_c-1$ message from the variable is erasure is:**
    $$q_t = 1- (1-p_t)^{d_c-1}$$
    - Combining the two equations we get:
    $$p_t = \epsilon (1-(1-p_{t-1})^{d_c-1})^{d_v-1}$$
    - Plotting for a rate 1/2 code with $d_v=3, d_c=6$, the following graph of $p_t \text{ against } t$ is shown:
    ![Capture](/assets/Capture_u4ff5ztia.PNG)

  - **Irregular Codes Density Evolution**
    - Recall that a node with degree $i$ means there are $i$ edges (nodes) connected to it
    - $\lambda_i$ is the fraction of edges connected to degree-i variable nodes
    - $\rho_i$ is the fraction of edges connected to degree-i check nodes
    - To get the density evolution, need to sum the probabilities of erasure transmission over all degrees (same degree nodes have the exact same probability of erasure transmission)
    - The probability that a message from variable to check node of degree-i being an erasure in step t is $p_{t,i} = \epsilon q_{t-1}^{i-1}$. Hence the probability of a **randomly picked edge** has $m_{vc}=?$ is:
    $$p_t = \sum_i \lambda_i p_{t,i} = \epsilon \sum_i \lambda_i q_{t-1}^{i-1} = \epsilon \lambda(q_{t-1})$$
    - The probability that an outgoing message from check node with degree j is erased in step t is $q_{t,j} = 1-(1-p_t)^{j-1}$. Hence the probability that a randomly picked edge has $m_{cv}=?$ is
    $$q_t = \sum_j \rho_jq_{t,j} = 1- \sum_j \rho_j (1-p_t)^{j-1} 1- \rho(1-p_t)$$
    - Combining two equations we arrive at:
    $$p_t = \epsilon \lambda(1-\rho(1-p_{t-1}))$$
    Which cannot be optimized easily over $\lambda(x), \rho(x)$. Other research papers have shown that a rate approaching Shannon limit can be achieved using appropriate optimization method.
### L7.3 LDPC Codes For General Binary Input Channels
Two other channels have yet to be discussed, the BSC(p) channel, with bit flip probability $p$, and the B-AWGN Channel: $Y=X+N$, where the input $X\in\{+1,-1\}$ and $N \in \mathcal{N}(0, \sigma^2)$, mapping the 0 code bit to $X=+1$, 1 code bit to $X=-1$
- **Optimal decoding for BSC Channels:**
The **block-wise** optimal decoding rule for BSC Channel is the most likely code word given the transmitted word, due to all the code words are equally likely, we can also write:
![Capture](/assets/Capture_w504jm3yq.PNG)
Therefore the probability of a received corrupt word given the codeword is probability of bits flipped times the probability of bits not flipped:
![Capture](/assets/Capture_bnu7nxrfn.PNG)
The **bit-wise** optimal decoding rule for BSC channel is for each jth bit in codeword:
![Capture](/assets/Capture_m12lqlz32.PNG)
