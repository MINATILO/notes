# 3F1 Information Theory and Coding[^1]
[^1]:Created by: Tom Xiaoding  Lu on 10/07/18
## L1. Probability Review
### 1.1 Random Variables
* A random variable (rv) $X$ is a function that maps outcome of experiment to value in set $\mathcal{X}$
* It is characterised by a probability mass function (pmf), $P_X(x)$ is the *realisation* of the rv $X$
* It has a cumulative distribution function (cdf), $F_X(a)=Pr(X\leq a)=\sum_{x\leq a}P_X(x)$
* Expected value is like the center of mass of the pmf, $\mathcal{E}(X)=\sum_a aP_X(a)$
* Variance is $\mathcal{E}[(X-\mathcal{E}X)^2]$, think of it as the expectation of the deviation from the mean
* $\mathcal{E}[(X-\mathcal{E}X)^2] = \mathcal{E}[X^2-2\mathcal{E}X+(\mathcal{E}X)^2]=\mathcal{E}(X^2)-2\mathcal{E}\mathcal{E}(X)+(\mathcal{E}X)^2=\mathcal{E}(X^2)-(\mathcal{E}X)^2$
* For any constant $a$, $\textrm{Var}(aX)=a^2\textrm{Var}(X)$
* Often we take expectations of functions of rvs, e.g. $\mathcal{E}[g(x)]=\sum_ag(a)P_X(a)$
### 1.2 Jointly Distributed Discrete rvs
Characterised by *joint pmf* $P_{XY}(x,y),\;\;x\in X,y\in Y$
*Marginal distributions:* $P_X$ can be computed through summation over $Y$:
$$
P_X(x)=\sum_y P_{XY}(x,y)
$$
*Conditional distribution* of $Y$ given $X$, we divide the joint distribution by what we already know (the distribution of $X$):
$$
P_{Y|X}(y|x)=\dfrac{P_{XY}(x,y)}{P_X(x)}
$$
**Product rule** is simply a rule derived from following the probability tree, think of it as generating a chain of probabilities:
$$
P_{XYZ}=P_X P_{Y|X} P_{Z|XY}=P_Y P_{X|Y} P_{Z|XY}\; \textrm{,etc}
$$
**Sum rule** is just a generalised form of marginalisation in multivariables:
$$
P_X(x)=\sum_y \sum_z P_{XYZ}(x,y,z)
$$
**Independence** is achieved within discrete random variables $X_1,X_2,...,X_n$ if and only if:
$$
P_{X1...X_n}(x1,...,x_n)=\prod_{j=1}^{n} P_{X_j}(x_j)
$$
Recall that in product rule, we can write the equation above as a generating function:
$$
P_{X_1,...,X_n}(x_1,...,x_n)=P_{X_1}(x_1)P_{X_2|X_1}(x_2|x_1)...P_{X_n|X_{n-1}...X_1}(x_n|x_{n-1}...x_1)
$$
Thus $X_1,X_2,...,X_n$ only when
$$
P_{X_i|\{X_j\}_{j\neq i}}= P_{X_i}
$$
### 1.3 Continuous Random Variables
* Characterised by the joint *density* function $f_{XY}(x,y)$
* $Pr(a\leq X \leq b, c\leq Y \leq d)=\int_{x=a}^{b}\int_{y=c}^{d}f_{XY}(x,y)dxdy$
* Conditional density, product and sum rules are analogous to discrete case, just replace the sum with integrals
## L2. Entropy Review
Entropy can be thought as the minimum number of bits required to encode some events that follow a probability distribution. The entropy of a discrete random variable $X$ with pmf $P$ is:
$$
H(X)=\sum_{x}P(x)\log \dfrac{1}{P(x)}
$$
### 2.1 Binary Entropy Function
$X$ is called a Bernoulli(p) random variable if takes value 1 with probability $p$ and $0$ with probability $1-p$. Intuitively, it is most uncertain when $p=0.5$ and has zero entropy at $p=\{0,1\}$. Its entropy's analytical equation is simply:
$$
H_2(p)=p\log \dfrac{1}{p} + (1-p)\log \dfrac{1}{1-p}
$$
This is plotted below:
<center>

![bernouli](/assets/bernouli.jpg)
</center>
