# 3F7 Information Theory and Coding[^1]
[^1]:Created by: Tom Xiaoding  Lu on 10/07/18
## L1. Probability Review
### 1.1 Random Variables
* A random variable (rv) $X$ is a function that maps outcome of experiment to value in set $\mathcal{X}$
* It is characterised by a probability mass function (pmf), $P_X(x)$ is the *realisation* of the rv $X$
* It has a cumulative distribution function (cdf), $F_X(a)=Pr(X\leq a)=\sum_{x\leq a}P_X(x)$
* Expected value is like the center of mass of the pmf, $\mathcal{E}(X)=\sum_a aP_X(a)$
* Variance is $\mathcal{E}[(X-\mathcal{E}X)^2]$, think of it as the expectation of the deviation from the mean
* $\mathcal{E}[(X-\mathcal{E}X)^2] = \mathcal{E}[X^2-2X\mathcal{E}(X)+(\mathcal{E}X)^2]=\mathcal{E}(X^2)-2\mathcal{E}(X)\mathcal{E}(X)+(\mathcal{E}X)^2=\mathcal{E}(X^2)-(\mathcal{E}X)^2$
* For any constant $a$, $\textrm{Var}(aX)=a^2\textrm{Var}(X)$
* Often we take expectations of functions of rvs, e.g. $\mathcal{E}[g(x)]=\sum_ag(a)P_X(a)$
### 1.2 Jointly Distributed Discrete rvs
Characterised by *joint pmf* $P_{XY}(x,y),\;\;x\in X,y\in Y$
*Marginal distributions:* $P_X$ can be computed through summation over $Y$:
$$
P_X(x)=\sum_y P_{XY}(x,y)
$$
*Conditional distribution* of $Y$ given $X$, we divide the joint distribution by what we already know (the distribution of $X$):
$$
P_{Y|X}(y|x)=\dfrac{P_{XY}(x,y)}{P_X(x)}
$$
**Product rule** is simply a rule derived from following the probability tree, think of it as generating a chain of probabilities:
$$
P_{XYZ}=P_X P_{Y|X} P_{Z|XY}=P_Y P_{X|Y} P_{Z|XY}\; \textrm{,etc}
$$
**Sum rule** is just a generalised form of marginalisation in multivariables:
$$
P_X(x)=\sum_y \sum_z P_{XYZ}(x,y,z)
$$
**Independence** is achieved within discrete random variables $X_1,X_2,...,X_n$ if and only if:
$$
P_{X1...X_n}(x1,...,x_n)=\prod_{j=1}^{n} P_{X_j}(x_j)
$$
Recall that in product rule, we can write the equation above as a generating function:
$$
P_{X_1,...,X_n}(x_1,...,x_n)=P_{X_1}(x_1)P_{X_2|X_1}(x_2|x_1)...P_{X_n|X_{n-1}...X_1}(x_n|x_{n-1}...x_1)
$$
Thus $X_1,X_2,...,X_n$ only when
$$
P_{X_i|\{X_j\}_{j\neq i}}= P_{X_i}
$$
### 1.3 Continuous Random Variables
* Characterised by the joint *density* function $f_{XY}(x,y)$
* $Pr(a\leq X \leq b, c\leq Y \leq d)=\int_{x=a}^{b}\int_{y=c}^{d}f_{XY}(x,y)dxdy$
* Conditional density, product and sum rules are analogous to discrete case, just replace the sum with integrals
## L2. Entropy Review
Entropy can be thought as the minimum number of bits required to encode some events that follow a probability distribution. The entropy of a discrete random variable $X$ with pmf $P$ is:
$$
H(X)=\sum_{x}P(x)\log \dfrac{1}{P(x)}
$$
### 2.1 Binary Entropy Function
$X$ is called a Bernoulli(p) random variable if takes value 1 with probability $p$ and $0$ with probability $1-p$. Intuitively, it is most uncertain when $p=0.5$ and has zero entropy at $p=\{0,1\}$. Its entropy's analytical equation is simply:
$$
H_2(p)=p\log \dfrac{1}{p} + (1-p)\log \dfrac{1}{1-p}
$$
This is plotted below:
<center>

![bernouli](/assets/bernouli.jpg)
</center>
### 2.2 Properties of Entropy
Putting it to words, the entropy of a random variable is **non negative** and bounded by the entropy of a uniform distribution.
Let $X$ be discrete random variable taking values in $\mathcal{X}$. Denote the alphabet size $|\mathcal{X}|$ by $M$. Then:
* $H(X)\geq 0$
* $H(X)\leq \log M$
* Among all random variables taking values in $\mathcal{X}$, the uniform distribution has the maximum entropy equal to $\log M$
#### 2.2.1 Greater than 0 proof
For any $x\in X,\; 0\leq P(x) \leq 1$, so $\dfrac{1}{P(x)}\geq 1$, therefore $\log \dfrac{1}{P(x)}\geq 1$ Hence:
$$
H(X) = \sum_{x}P(x)\log \dfrac{1}{P(x)}\geq 1
$$
#### 2.2.2 Upper Bound Proof
Need to use the property $\ln x \leq  (x-1)$, for $x > 0$ with equality iff $x=1$
$$
H(X)-\log M = \sum_xP(x)\log \dfrac{1}{P(x)}-\sum_xP(x)\log M
$$
$$
=\dfrac{1}{\ln2}\sum_xP(x) \ln \dfrac{1}{MP(x)} \leq \dfrac{1}{\ln2}\sum_xP(x)  \left(\dfrac{1}{MP(x)}-1\right)
$$
$$
= \dfrac{1}{\ln2}\left(\sum_x \dfrac{1}{M}- \sum_xP(x)\right)=0
$$
We also know that for equality to happen $\dfrac{1}{MP(x)}=1$ therefore $P(x)=\dfrac{1}{M}$
### 2.3 Joint and Conditional Entropy
For the **joint entropy** of discrete rvs $X$ and $Y$, think of the matrix of the probability mass function $P_{XY}$, its entropy is just the sum of the elements times the minus log of the elements within this probability matrix:
$$
H(X,Y)=\sum_{x,y}P_{XY}(x,y)\log \dfrac{1}{P_{XY}(x,y)}
$$
The **conditional entropy** of $Y$ given $X$ is sort of counter intuitive written in its raw form:
$$
H(Y|X)=\sum_{x,y}P_{XY}(x,y) \log \dfrac{1}{P_{Y|X}(y|x)}
$$
Let us try to prove its relationship with the joint entropy to gain some intuiation:
$$
H(Y|X)=\sum_{x,y}P_{XY}(x,y) \log \dfrac{P_X(x)}{P_{XY}(x,y)}
$$
$$
H(X) =  \sum_{x}P_X(x)\log \dfrac{1}{P_X(x)} =  \sum_{x,y}P_{XY}(x,y)\log \dfrac{1}{P_X(x)}
$$
$$
H(X)+H(Y|X)=  \sum_{x,y} P_{XY}(x,y) \left( \log \dfrac{P_X(x)}{P_{XY}(x,y)}+\log \dfrac{1}{P_X(x)} \right) =  \sum_{x,y} P_{XY}(x,y) \log \dfrac{1}{P_{XY}(x,y)} = H(X,Y)
$$
Hence, we arrive at a more intuitive result where uncertainty (entropy) of $Y$ given $X$ is equal to the joint uncertainty minus the uncertainty in $X$ since it is "known".
$$
H(Y|X)=H(X,Y)-H(X)
$$
Also the conditional entropy $H(Y|X)$ can also be written as the *average* uncertainty in $Y$ given $X$ using product rule:
$$
H(Y|X)=\sum_{x}P_X(x)\sum_y P_{Y|X}(y|x) \log \dfrac{1}{P_{Y|X}(y|x)} = \sum_{x}P_X(x)H(Y|X=x)
$$
### 2.4 Joint Entropy of Multiple RVs
The **joint entropy** of $X_1,X_2,...,X_n$ is:
$$
H(X_1,X_2,...,X_n) = \sum_{x_1,...,x_n}P_{X_1,...X_n}(x_1,...x_n)\log\dfrac{1}{P_{X_1,...X_n}(x_1,...x_n)}
$$
Which can be written in a generative form, interpreted as total uncertainty is uncertainty in $X_1$ plus uncertainty in $X_2$ knowing $X_1$, plus uncertainty in $X_3$ knowing both $X_1$ and $X_2$ and so on:
$$
H(X_1,X_2,...,X_n) = H(X_1)+H(X_2|X_2)+H(X_3|X_1,X_2)+...+H(X_n|X_1,...X_{n-1})
$$
Equation above can be written in a condensed format:
$$
H(X_1,X_2,...,X_n) = \sum_{i=1}^{n}H(X_i|X_{i-1},...,X_1)
$$
If $X_1,X_2,...,X_n$ are all independent then the joint entropy is the sum of their individual entropy:
$$
H(X_1,X_2,...,X_n) = \sum_{i}^{n}H(X_i)
$$

## L3. Law of Large Numbers
### 3.1 Estimating Tail Probabilities
We often want to bound the probability of unlikely events, sometimes we do not know the distribution of the rv, but we can still get a bound for it using:
* **Markov's inequality** for **non negative rvs** which requires only mean
* **Chebyshev's inequality** which requires both mean and variance (for general rvs)

#### 3.1.1 Markov's Inequality
**Mechanical Intuiation:**
Let us imagine the probability mass function being some sort of shape, this is especially valid since we have only positive $rv$s:
<center>

![DeepinScreenshot_select-area_20181018173734](/assets/DeepinScreenshot_select-area_20181018173734.png)</center>
</center>

So the center of mass is at $\mu$. Let us denote a region of mass $M$ contributed by axis $[a, \infty)$, we can also write $M$ as:
$$
M = P(X\geq a)
$$
We can write down with that $\mu \geq Ma$ since at best we have $M$ being a point mass that lies exactly $\mu$, otherwise it will bring the center of mass up. Therefore we arrive at the Markov's inequality:
$$
P(X\geq a)\leq \dfrac{\mathcal{E}[X]}{a}
$$
**Proof of Markov's Inequality:**
We start with the definition of expectation and break it into two constituents:
$$
\mathcal{E}[X]=\sum_{r\geq 0}rP(X=r) = \sum_{r< a}rP(X=r)+ \sum_{r\geq a}rP(X=r)
$$
Since for the part where $r\geq a$, we can write $\sum_{r\geq a}rP(X=r) \geq \sum_{r\geq a}aP(X=r)$, therefore:
$$
\mathcal{E}[X] \geq \sum_{r< a}rP(X=r) + \sum_{r\geq a}aP(X=r)
$$
$$
\mathcal{E}[X] \geq a \sum_{r\geq a}P(X=r) = a P(X\geq a)
$$

#### 3.1.2 Chebyshev's Inequality:
Often, we need to bound the tail probabilities of *deviations* around the mean of an *rv*, i.e. working out the margin of safety using:
$$
P(|X- \mathcal{E}[X]|\geq a), \;\; \textrm{for some }a>0
$$
**Chevyshev's inequality is a way to bound the probability of deviations around the mean** with knowledge of just the *mean* and *variance* $\sigma^2$.
$$
P(|X- \mathcal{E}[X]|\geq a) \leq \dfrac{\sigma^2}{a^2}
$$
**Mechanical Intuition:**
In order to build some sort of intuition for it, we imagine our probability mass function being some sort of physical shape:
<center>

![DeepinScreenshot_select-area_20181018165552](/assets/DeepinScreenshot_select-area_20181018165552.png)
</center>

Looking at the picture above, we say that the variance $\sigma ^2 = \mathcal{E}[(X-\mu)^2] = \int_{-\infty}^{\infty}dx f(x)(\mu-x)^2 $ is interpreted as the *moment of inertia* of our "shape" spinning about the center of rotation $\mu$. If we denote $M$ being the total mass contributed by the region $(-\infty,\mu-k\sigma] \cup [\mu+k\sigma, \infty)$ as shown, we have:
$$
M = P(|X- \mu|\geq k\sigma) = P(|X- \mathcal{E}[X]|\geq k\sigma)
$$
Now, we know when this shape is spinning, to calculate its moment of inertia, our region $M$ will contribute at least $M (k\sigma)^2$ by imagine them being point masses distance $k\sigma$ away from the centre $\mu$. Therefore we can write down:
$$
\sigma^2 \geq M (k\sigma)^2
$$
Hence
$$
1 \geq M k^2 = P(|X- \mathcal{E}[X]|\geq k\sigma) k^2
$$
We arrive at the alternative form of *Chebyshev's* inequality (origional form is obtained through substituting $a=k\sigma$):
$$
P(|X- \mathcal{E}[X]|\geq k\sigma) \leq \dfrac{1}{k^2}
$$

**Formal Proof**:
Note that $$P(|X-\mu|\geq a) = P(|X-\mu|^2 \geq  a^2)$$
Let $Y = |X-\mu|^2$, since $Y$ is a non-negative rv, Markov's inequality can be applied:
$$
P(Y\geq a^2)\leq \dfrac{\mathcal{E}[Y]}{a^2} \rightarrow P(|X-\mu|\geq a) \leq  \dfrac{\textrm{Var}(X)}{a^2}
$$
### 3.2 Weak Law of Large Numbers
Very intuitive, as it basically states empirical average converges to mean.
Let $X_1,X_2, ...$ be a sequence of i.i.d (independent and identically distributed) rvs with finite mean $\mu$. Let $S_n=\dfrac{1}{n}\sum_{i=1}^n X_i$ then **informally**:
$$
\lim_{n\rightarrow \infty}S_n\rightarrow\mu
$$
Formally:
$$
\lim_{n\rightarrow \infty}P(|S_n-\mu|\geq \epsilon)=0
$$
**Proof**:
Using Chebyshev's inequality we write:
$$
P(|S_n-\mu|\geq \epsilon)\leq \dfrac{\textrm{Var}(S_n)}{\epsilon^2}
$$
Where  $\textrm{Var}(S_n) = \dfrac{1}{n^2}\textrm{Var}(\sum_i X_i)^*  = \dfrac{1}{n^2}\sum_i \textrm{Var}(X_i)= \dfrac{n\sigma^2}{n^2} $
Hence:
$$
P(|S_n-\mu|\geq \epsilon)\leq \dfrac{\sigma^2}{n^2}
$$
Taking the limit to $n\rightarrow\infty$ we have $P(|S_n-\mu|\geq \epsilon)=0$

\*I was quiet sceptical about the line $\textrm{Var}(\sum_i X_i)=\sum_i \textrm{Var}(X_i)$, this intuitively does make sense for i.i.d rvs (total uncertainty equals to sum of individual uncertainty contributions) but lets still prove it, for two i.i.d rvs $X$ and $Y$:
$$
\textrm{Var}(X+Y)=\mathcal{E}[(X+Y)^2-(\mathcal{E}[X+Y])^2]=\mathcal{E}[(X+Y)^2-(\mathcal{E}[X]+\mathcal{E}[Y])^2]
$$
$$
\textrm{Var}(X+Y)=\mathcal{E}[X^2+Y^2+2XY-\mathcal{E}[X]^2-\mathcal{E}[Y]^2-2\mathcal{E}[X]\mathcal{E}[Y]]
$$
$$
\textrm{Var}(X+Y)=\mathcal{E}[X^2]+\mathcal{E}[Y^2]+2\mathcal{E}[XY]-\mathcal{E}[X]^2-\mathcal{E}[Y]^2-2\mathcal{E}[XY]
$$
$$
\textrm{Var}(X+Y)=\textrm{Var}(X)+\textrm{Var}(Y)
$$
### 3.3 Asymptotic Equipartition Property (AEP)
This property gives us more insight the meaning of entropy. We have the intuition that for whatever r.v, if we generate a long sequence of it, the probability of different sequences should be the same. i.e let the rv be english words, an article being the generated sequence of such rvs, the note I am typing now has basically the same existing probability as a wikipedia article on dolphins! (This example is not so true since each word is dependent on the previous, but it demonstrates the idea).
But what does this probability tends towards? A very small number, but how small? The property of AEP dictates exactly how small this probability is, if $X_1,X_2,...$ are i.i.d rvs :
$$
\lim_{n\rightarrow \infty}\dfrac{-1}{n}\log P_X(X_1,X_2,...X_n)=H(X)
$$
Or in words: the log likelihood of a sequence tends towards its entropy for large sequences.

**Proof (Badly explained here):**
This is basically an extension of WLLN where $S_n=\dfrac{1}{n} \sum_i Y_i$:
$$
\lim_{n\rightarrow\infty}P(|S_n-\mathcal{E}[Y_i]|\geq \epsilon)=0\rightarrow  P(|S_n-\mathcal{E}[Y_i]| < \epsilon)=1
$$
From the log of the generated sequence $\log P_X(X1,X_2,...X_n)$ we have:
$$
\log \prod_{i=1}^n P_X(X_i) = \sum_{i=1}^n \log P_X(X_i)
$$
Let $\;\; -\sum_{i=1}\log P_X(X_i) = \sum_{i=1}Y_i\;$ and substitute into WWLN we get:

$$
\lim_{n\rightarrow\infty}P(|\dfrac{-1}{n}\log P_X(X_1,X_2,...X_n)-\mathcal{E}[Y_1]|< \epsilon)=1
$$
Noting the expectation of $Y_1=\log P_X(X_1)$ is $H(X)$ therefore:
$$
\lim_{n\rightarrow\infty}P\left(\left|\dfrac{-1}{n}\log P_X(X_1,X_2,...X_n)-H(X)\right|< \epsilon\right)=1
$$

## L4. Prefix-Free Codes and Kraft Inequality
### 4.1 Prefix-Free Codes
A code is *prefix-free* if **no codeword is the prefix of another**, i.e. {0,10,11} is a prefix code, {00,010,011,0101,1} is not since 010 is a prefix for 0101. In order to decode a prefix code, we would have to use letter spaces.
Graphically, the prefix condition on a tree has **codewords on the leaves of the tree** in the diagram below nodes that are coloured are the codewords:
<center>

![DeepinScreenshot_select-area_20181020221902](/assets/DeepinScreenshot_select-area_20181020221902.png)
</center>

### 4.2 Kraft Inequality
* A full tree of depth $l_{\max}$ has $2^{l_{\max}}$ leaves at that depth level
* Whenever a codeword of lenthg $l<l_{\max}$ is created, all its descendants are removed from the tree

<center>

![DeepinScreenshot_select-area_20181020223121](/assets/DeepinScreenshot_select-area_20181020223121.png)
</center>

* Because a codeword **must** be a leaf which is at the termination of a node. Therefore each codeword of length $l$ leads to a reduction of $2^{l-l_{\max}}$ leaves at the maximum depth $l_{\max}$
* Hence the total number of unusable leaves at level $l_{\max}$ for a code with word length {$l_1,l_2,...l_N$} is $\sum_{i=1}^N 2^{l_{max}-l_i}$
* We arrive at the Kraft inequality which is a necessary condition for any prefix-free code with lengths {$l_1,...l_N$}:
$$
\sum_{i=1}^N 2^{l_{max}-l_i} \leq 2^{l_{\max}} \quad \rightarrow \quad \sum_{i=1}^N 2^{-l_i} \leq 1
$$

### 4.3 Efficient Coding Theorem
The expected codeword length $L$ is simply $L=\sum_{x\in \mathcal{X}} p(x)l(x)$ letting $X$ be a random variable taking values in the alphabet $\mathcal{X}$ with entropy $H(X)$. Then for any binary prefix-free code of $X$ satisfies:
$$
L \geq H(X)
$$
To prove this, we need to show that $H(X)-L \leq 0$:
<center>

![DeepinScreenshot_select-area_20181020230050](/assets/DeepinScreenshot_select-area_20181020230050.png)
</center>

where $(a)$ is obtained using inequality $\ln(x)\leq x-1$ and $(b)$ uses Kraft inequality.
For blocks of source symbols, if $X$ is an iid source, denoting $X^N = \{X_1,X_2,...,X_N\}$ then:
$$
H(X^N) = NH(X) \quad \rightarrow \quad \dfrac{\mathcal{E}[l(X^N)]}{N}\geq H(X)
$$























