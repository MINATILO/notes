# 3F7 Information Theory and Coding[^1]
[^1]:Created by: Tom Xiaoding  Lu on 10/07/18
## L1. Probability Review
### 1.1 Random Variables
* A random variable (rv) $X$ is a function that maps outcome of experiment to value in set $\mathcal{X}$
* It is characterised by a probability mass function (pmf), $P_X(x)$ is the *realisation* of the rv $X$
* It has a cumulative distribution function (cdf), $F_X(a)=Pr(X\leq a)=\sum_{x\leq a}P_X(x)$
* Expected value is like the center of mass of the pmf, $\mathcal{E}(X)=\sum_a aP_X(a)$
* Variance is $\mathcal{E}[(X-\mathcal{E}X)^2]$, think of it as the expectation of the deviation from the mean
* $\mathcal{E}[(X-\mathcal{E}X)^2] = \mathcal{E}[X^2-2\mathcal{E}X+(\mathcal{E}X)^2]=\mathcal{E}(X^2)-2\mathcal{E}\mathcal{E}(X)+(\mathcal{E}X)^2=\mathcal{E}(X^2)-(\mathcal{E}X)^2$
* For any constant $a$, $\textrm{Var}(aX)=a^2\textrm{Var}(X)$
* Often we take expectations of functions of rvs, e.g. $\mathcal{E}[g(x)]=\sum_ag(a)P_X(a)$
### 1.2 Jointly Distributed Discrete rvs
Characterised by *joint pmf* $P_{XY}(x,y),\;\;x\in X,y\in Y$
*Marginal distributions:* $P_X$ can be computed through summation over $Y$:
$$
P_X(x)=\sum_y P_{XY}(x,y)
$$
*Conditional distribution* of $Y$ given $X$, we divide the joint distribution by what we already know (the distribution of $X$):
$$
P_{Y|X}(y|x)=\dfrac{P_{XY}(x,y)}{P_X(x)}
$$
**Product rule** is simply a rule derived from following the probability tree, think of it as generating a chain of probabilities:
$$
P_{XYZ}=P_X P_{Y|X} P_{Z|XY}=P_Y P_{X|Y} P_{Z|XY}\; \textrm{,etc}
$$
**Sum rule** is just a generalised form of marginalisation in multivariables:
$$
P_X(x)=\sum_y \sum_z P_{XYZ}(x,y,z)
$$
**Independence** is achieved within discrete random variables $X_1,X_2,...,X_n$ if and only if:
$$
P_{X1...X_n}(x1,...,x_n)=\prod_{j=1}^{n} P_{X_j}(x_j)
$$
Recall that in product rule, we can write the equation above as a generating function:
$$
P_{X_1,...,X_n}(x_1,...,x_n)=P_{X_1}(x_1)P_{X_2|X_1}(x_2|x_1)...P_{X_n|X_{n-1}...X_1}(x_n|x_{n-1}...x_1)
$$
Thus $X_1,X_2,...,X_n$ only when
$$
P_{X_i|\{X_j\}_{j\neq i}}= P_{X_i}
$$
### 1.3 Continuous Random Variables
* Characterised by the joint *density* function $f_{XY}(x,y)$
* $Pr(a\leq X \leq b, c\leq Y \leq d)=\int_{x=a}^{b}\int_{y=c}^{d}f_{XY}(x,y)dxdy$
* Conditional density, product and sum rules are analogous to discrete case, just replace the sum with integrals
## L2. Entropy Review
Entropy can be thought as the minimum number of bits required to encode some events that follow a probability distribution. The entropy of a discrete random variable $X$ with pmf $P$ is:
$$
H(X)=\sum_{x}P(x)\log \dfrac{1}{P(x)}
$$
### 2.1 Binary Entropy Function
$X$ is called a Bernoulli(p) random variable if takes value 1 with probability $p$ and $0$ with probability $1-p$. Intuitively, it is most uncertain when $p=0.5$ and has zero entropy at $p=\{0,1\}$. Its entropy's analytical equation is simply:
$$
H_2(p)=p\log \dfrac{1}{p} + (1-p)\log \dfrac{1}{1-p}
$$
This is plotted below:
<center>

![bernouli](/assets/bernouli.jpg)
</center>
### 2.2 Properties of Entropy
Putting it to words, the entropy of a random variable is **non negative** and bounded by the entropy of a uniform distribution.
Let $X$ be discrete random variable taking values in $\mathcal{X}$. Denote the alphabet size $|\mathcal{X}|$ by $M$. Then:
* $H(X)\geq 0$
* $H(X)\leq \log M$
* Among all random variables taking values in $\mathcal{X}$, the uniform distribution has the maximum entropy equal to $\log M$
#### 2.2.1 Greater than 0 proof
For any $x\in X,\; 0\leq P(x) \leq 1$, so $\dfrac{1}{P(x)}\geq 1$, therefore $\log \dfrac{1}{P(x)}\geq 1$ Hence:
$$
H(X) = \sum_{x}P(x)\log \dfrac{1}{P(x)}\geq 1
$$
#### 2.2.2 Upper Bound Proof
Need to use the property $\ln x \leq  (x-1)$, for $x > 0$ with equality iff $x=1$
$$
H(X)-\log M = \sum_xP(x)\log \dfrac{1}{P(x)}-\sum_xP(x)\log M
$$
$$
=\dfrac{1}{\ln2}\sum_xP(x) \ln \dfrac{1}{MP(x)} \leq \dfrac{1}{\ln2}\sum_xP(x)  \left(\dfrac{1}{MP(x)}-1\right)
$$
$$
= \dfrac{1}{\ln2}\left(\sum_x \dfrac{1}{M}- \sum_xP(x)\right)=0
$$
We also know that for equality to happen $\dfrac{1}{MP(x)}=1$ therefore $P(x)=\dfrac{1}{M}$
### 2.3 Joint and Conditional Entropy
For the **joint entropy** of discrete rvs $X$ and $Y$, think of the matrix of the probability mass function $P_{XY}$, its entropy is just the sum of the elements times the minus log of the elements within this probability matrix:
$$
H(X,Y)=\sum_{x,y}P_{XY}(x,y)\log \dfrac{1}{P_{XY}(x,y)}
$$
The **conditional entropy** of $Y$ given $X$ is sort of counter intuitive written in its row form:
$$
H(Y|X)=\sum_{x,y}P_{XY}(x,y) \log \dfrac{1}{P_{Y|X}(y|x)}
$$
Let us try to prove its relationship with the joint entropy to gain some intuiation:
$$
H(Y|X)=\sum_{x,y}P_{XY}(x,y) \log \dfrac{P_X(x)}{P_{XY}(x,y)}
$$
$$
H(X) =  \sum_{x}P_X(x)\log \dfrac{1}{P_X(x)} =  \sum_{x,y}P_{XY}(x,y)\log \dfrac{1}{P_X(x)}
$$
$$
H(X)+H(Y|X)=  \sum_{x,y} P_{XY}(x,y) \left( \log \dfrac{P_X(x)}{P_{XY}(x,y)}+\log \dfrac{1}{P_X(x)} \right) =  \sum_{x,y} P_{XY}(x,y) \log \dfrac{1}{P_{XY}(x,y)} = H(X,Y)
$$
Hence, we arrive at a more intuitive result where uncertainty (entropy) of $Y$ given $X$ is equal to the joint uncertainty minus the uncertainty in $X$ since it is "known".
$$
H(Y|X)=H(X,Y)-H(X)
$$
