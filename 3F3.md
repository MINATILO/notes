# 3F3 Statistical Signal Processing[^1]

[^1]:Created by: Tom Xiaoding  Lu on 10/28/18

##1. Foundation of Probability and Random Variables
###1.1 Axioms of Probability
A probability $P$ (which is a mapping function) assigns each event $E$, $E \subset \Omega$, a number in the interval $[0,1]$ and $P$ must satisfy the following properties:
* $P(\Omega)=1$
* For events $A,B$ such that $A\cap B = \emptyset$ (i.e. disjoint) then $P(A\cup B) = P(A)+P(B)$
* If $A_1,A_2,...$ are disjoint then $P(\cap_{i=1}^{\infty}A_i) = \sum_{i=1}^\infty P(A_i)$

### 1.2 Indicator Function
Define the indicator function for and event $A$,
$$
\mathbb{I}_A(t) = \begin{cases}
1 \;\;\; if \;\;\; t\in A\\
0 \;\;\; if \;\;\; t\not\in A
\end{cases}
$$
i.e. let $\Omega$ be a finite discrete set, i.e. $\Omega = \{\omega_1,\omega_2,...,\omega_n\}$, let $p_1,p_2,...,p_n$ be non-negative numbers that add to 1, for any event $A$, set
$$
P(A) = \sum_{i=1}^n \mathbb{I}_A(\omega_i)p_i
$$
### 1.3 Random Variables
Given a probability space $(\Omega, P)$, a random variable is a function $X(\omega)$ which maps each element $\omega$ of the sample space $\Omega$ onto a point on the real line.
i.e. Flip a coin twice, the sample space is $\Omega = \{(T,T), (T,H),(H,T) ,(H,H)\}$. Let $X(\omega)$ be the number of heads

### 1.4 CDF Method of Variables Transformation
Let $X$ have pdf $f_X$ and cdf $F_X$. Define the new random variable $Y=g(X)$, the pdf $f_Y$ is found as follows:
Since:
$$
F_Y(y) = P(Y\leq y) = P(g(X)\leq y)
$$
If $g(X)$ follows a one to one mapping and is none decreasing then:
$$
F_Y(y) = P(X \leq g^{-1}(y)) = F_X(g^{-1}(y))
$$
Therefore through the definition of pdf and chain rule:
$$
f_Y(y) = f_X(g^{-1}(y))\dfrac{d}{dx}g^{-1}(y)
$$
It is shown (graphically later) that if the mapping is one to one but **decreasing** then:
$$
f_Y(y) = -f_X(g^{-1}(y))\dfrac{d}{dx}g^{-1}(y)
$$
But since the gradient is negative itself we can combine the result for both decreasing and increasing functions:
$$
f_Y(y) = f_X(g^{-1}(y))\left|\dfrac{d}{dx}g^{-1}(y)\right|
$$
The graphical representation of both increasing and decreasing $g(x)$ is shown below:
<center>

![Screenshot 2018-10-28 at 15.55.07](/assets/Screenshot%202018-10-28%20at%2015.55.07.png)
</center>

From the left it is seen that $P(Y\leq y)$ is equivalent to $P(X\leq g^{-1}(y))$, on the right figure $P(Y\leq y) = 1-P(X\leq g^{-1}(y))$
### 1.5 Sum of Two Independent Random Variables
**Method I**:
$$
f_{X_1,Y}(x_1,y) = f_1(x_1)f_{Y|X_1}(y|x_1)
$$
Since $Y=X_2+x_1$ therefore $f_{Y|X_1}(y)$
















































